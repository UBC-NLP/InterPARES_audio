{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bc2eba72",
   "metadata": {
    "id": "J1xGz1A4H3iA"
   },
   "source": [
    "<img src=\"https://dlnlp.ai/img/InterPARES_Audio.jpg\" alt=\"InterPARES_Audio.jpg\" width=\"35%\" height=\"25%\" align=\"right\"/>\n",
    "\n",
    "\n",
    "# üöÄ Multilingual Audio Analysis with InterPARES-Audio\n",
    "\n",
    "\n",
    "### **Description**\n",
    "\n",
    "This notebook provides an end-to-end pipeline to process a long audio file containing multiple speakers who may be speaking in different languages. It automates the following workflow:\n",
    "\n",
    "1.  **Speaker Diarization**: Identifies *who* spoke and *when*, using `pyannote.audio`.\n",
    "2.  **Multilingual Transcription**: Transcribes each speaker's segment, automatically detecting the language using `openai/whisper-large-v3`.\n",
    "3.  **LLM Analysis**: Uses a Large Language Model (`openai/gpt-oss-20b`) running on a remote, Ollama-compatible service to summarize the transcript, extract action items, and generate key insights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3f4d33",
   "metadata": {
    "id": "kFqA8K49IH1z"
   },
   "source": [
    "### **üõ†Ô∏è Step 1 - Setup & Installation**\n",
    "\n",
    "This cell installs all necessary Python libraries and configures the environment to use a GPU if one is available. A GPU is highly recommended for the performance-intensive models used in this pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "68369841-2287-42ac-a216-6769b17ee03c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install torch torchvision torchaudio\n",
    "# !pip install pyannote.audio langchain ollama\n",
    "# !pip install protobuf==3.20.3\n",
    "# !pip install \"numpy<2.0\"\n",
    "# !pip datasets\n",
    "# !pip install --upgrade ipywidgets\n",
    "# !pip install iso639\n",
    "# !pip install pdfkit\n",
    "# !pip install markdown2 weasyprint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405a7de4",
   "metadata": {
    "id": "aM5k-P0zIftF"
   },
   "source": [
    "### **üìö Step 2: Import Libraries and Load Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "085756aa-1886-495f-825d-7a2bced23a35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pyannote.audio import Pipeline as PyannotePipeline\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from transformers import pipeline\n",
    "from datasets import Audio\n",
    "from tqdm.auto import tqdm\n",
    "# --- Suppress the Tqdm warning about ipywidgets ---\n",
    "import warnings\n",
    "from tqdm import TqdmWarning\n",
    "warnings.filterwarnings(\"ignore\", category=TqdmWarning)\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "from transformers import pipeline, WhisperForConditionalGeneration, WhisperProcessor\n",
    "import torch\n",
    "torch.backends.cuda.matmul.allow_tf32 = True\n",
    "torch.backends.cudnn.allow_tf32 = True\n",
    "# Import iso639 for language name lookup\n",
    "from iso639 import Lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f997cbd",
   "metadata": {
    "id": "mE5q_iYdIJoU"
   },
   "outputs": [],
   "source": [
    "# Check GPUs usage\n",
    "#----------------------\n",
    "# Check if a GPU is available and set the device and data type accordingly\n",
    "if torch.cuda.is_available():\n",
    "    print(\"‚úÖ GPU is available. We will use the CUDA device.\")\n",
    "    DEVICE = \"cuda:0\"\n",
    "    TORCH_DTYPE = torch.float16\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è GPU not available. Using CPU. This will be very slow.\")\n",
    "    DEVICE = \"cpu\"\n",
    "    TORCH_DTYPE = torch.float32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccec3c01-8da0-4bb8-b31e-ba281308d538",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --- Hugging Face Configuration ---\n",
    "!huggingface-cli login --token <<<<YOUR_TOKEN>>>>\n",
    "# --- Cache Dir  ---\n",
    "# AUDIO_FILE =\"A02898.wav\"\n",
    "CACHE_DIR=\"path_to_your_cache_dir\"  # e.g. /content/cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c6d51d-ea61-4918-b764-9ec4b9be6d91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Part 1: Speaker Diarization (Language Agnostic) ---\n",
    "print(\"‚úÖ Loading Speaker Diarization Model\")\n",
    "diarization_pipeline = PyannotePipeline.from_pretrained(\n",
    "    \"pyannote/speaker-diarization-3.1\",\n",
    "    cache_dir=CACHE_DIR\n",
    ")\n",
    "diarization_pipeline.to(torch.device(DEVICE))\n",
    "\n",
    "print(\"‚úÖ Loading Whisper Model and Processor\")\n",
    "\n",
    "# --- Load model and processor ---\n",
    "MODEL_NAME = \"openai/whisper-large-v3\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "TORCH_DTYPE = torch.float16\n",
    "asr_pipeline = pipeline(\n",
    "    \"automatic-speech-recognition\",\n",
    "    model=MODEL_NAME,\n",
    "    torch_dtype=TORCH_DTYPE,\n",
    "    device=DEVICE,\n",
    ")\n",
    "processor = WhisperProcessor.from_pretrained(MODEL_NAME)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef03ea26",
   "metadata": {
    "id": "eZ5r_2tXIZcW"
   },
   "source": [
    "### **ü¶ô Step 3 - AI Service Connection**\n",
    "\n",
    "This notebook is configured to connect to a remote, Ollama-compatible AI service. All configuration, including the service URL and model name, is handled in the next cell. No local Ollama setup is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba4e449",
   "metadata": {
    "id": "j-2R0G_eIg2i"
   },
   "outputs": [],
   "source": [
    "# --- AI Service Configuration ---\n",
    "OLLAMA_SERVER = \"OLLAMA_SERVER\"  # e.g. \"http://localhost:11434\"\n",
    "LLM_MODEL_NAME = \"gpt-oss:20b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ee4f76-e026-4b93-824f-699c1b191b5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Setup Ollama Client for external service ---\n",
    "client = ollama.Client(host=OLLAMA_SERVER)\n",
    "print(f\"Connecting to AI service at {OLLAMA_SERVER} with model {LLM_MODEL_NAME}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4775cc13-9397-4d15-a455-1957c1ceea1c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "scrolled": true
   },
   "source": [
    "### üé§ **Step 4: Functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29ab349b-5bdc-40fa-ace1-032cf96e3c39",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Utterance Generation and Transcription Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40cfabb2-7b6b-46ca-9114-963db68824cf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def generate_utterances(audio_file):\n",
    "    diarization_output = diarization_pipeline(audio_file)\n",
    "    print(\"‚úÖ Diarization complete.\")\n",
    "    return diarization_output\n",
    "\n",
    "def transcribe(audio_file):\n",
    "    waveform, sr = torchaudio.load(audio_file)\n",
    "    TARGET_SR = 16000\n",
    "    if sr != TARGET_SR:\n",
    "        waveform = T.Resample(sr, TARGET_SR)(waveform)\n",
    "    waveform = waveform.squeeze()\n",
    "    \n",
    "    full_transcript = []\n",
    "    print(\"Transcribing segments for each speaker...\")\n",
    "    for turn, _, speaker in tqdm(diarization_output.speaker_diarization.itertracks(yield_label=True)):\n",
    "        start_time, end_time = turn.start, turn.end\n",
    "        segment = waveform[int(start_time * TARGET_SR):int(end_time * TARGET_SR)].numpy()\n",
    "        \n",
    "        # Step 1: Detect language using Whisper's encoder\n",
    "        input_features = processor(segment, return_tensors=\"pt\", sampling_rate=TARGET_SR).input_features.to(DEVICE, dtype=TORCH_DTYPE)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # Get language logits from the model\n",
    "            predicted_ids = asr_pipeline.model.detect_language(input_features)\n",
    "            \n",
    "            # Extract the language token ID\n",
    "            if isinstance(predicted_ids, tuple):\n",
    "                lang_token_id = predicted_ids[0].item()\n",
    "            else:\n",
    "                lang_token_id = predicted_ids.item() if predicted_ids.ndim == 0 else predicted_ids[0].item()\n",
    "        \n",
    "        # Decode the language token to get language code\n",
    "        lang_token = processor.tokenizer.decode([lang_token_id])\n",
    "        # Remove special token markers like <|en|>\n",
    "        lang = lang_token.strip(\"<|>\")\n",
    "        try:\n",
    "            lang_obj = Lang(lang)\n",
    "            lang_name = lang_obj.name\n",
    "        except (KeyError, AttributeError):\n",
    "            # Fallback if language code not found\n",
    "            lang_name = lang\n",
    "        \n",
    "        # Step 2: Transcribe with detected language\n",
    "        result = asr_pipeline(segment, generate_kwargs={\"language\": lang, \"task\": \"transcribe\",\n",
    "                                                       \"return_timestamps\": True  # Add this line\n",
    "                                                       })\n",
    "        text = result[\"text\"].strip()\n",
    "        \n",
    "        full_transcript.append({\n",
    "            \"start\": start_time,\n",
    "            \"speaker\": speaker,\n",
    "            \"lang_code\": lang,\n",
    "            \"lang_name\": lang_name,\n",
    "            \"text\": text\n",
    "        })\n",
    "               \n",
    "        # break\n",
    "    \n",
    "    # --- Format output ---\n",
    "    final_text = \"\\n\".join(\n",
    "        f\"[{u['start']:.2f}s] ({u['lang_code']} ‚Äì {u['lang_name']}) {u['speaker']}: {u['text']}\"\n",
    "        for u in full_transcript\n",
    "    )\n",
    "    print(\"\\n--- ‚úÖ Generate full transcript with language detection ---\")\n",
    "    return final_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae11687e-dc60-4770-aa62-b30e9d21126b",
   "metadata": {},
   "source": [
    "### Report Generation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "68275453-ad44-4d91-813a-90a35606715b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def estimate_tokens(text):\n",
    "    \"\"\"Estimate token count (rough approximation: 1 token ‚âà 4 characters)\"\"\"\n",
    "    return len(text) // 4\n",
    "\n",
    "\n",
    "def generate_meeting_report(final_transcript_text, client, model_name=\"gpt-oss:20b\", \n",
    "                           context_window=8192, reserved_tokens=1500, \n",
    "                           audio_filename=None, generated_by=None, target_language=\"English\"):\n",
    "    \"\"\"\n",
    "    Generate a comprehensive meeting analysis report from transcript.\n",
    "    \n",
    "    Args:\n",
    "        full_transcript (list): List of dicts with keys: 'start', 'speaker', 'lang_name', 'text'\n",
    "        client: Ollama client instance\n",
    "        model_name (str): Name of the Ollama model to use\n",
    "        context_window (int): Model's context window size in tokens\n",
    "        reserved_tokens (int): Tokens to reserve for prompts and response\n",
    "        audio_filename (str): Original audio file name (optional)\n",
    "        generated_by (str): Name/identifier of person generating report (optional)\n",
    "        target_language (str): Target language for the report (default: \"English\")\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (final_report (str), transcript_tokens (int), processing_method (str), metadata (dict))\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Generating Meeting Analysis Report ---\")\n",
    "    \n",
    "    # # Format the final transcript for LLM analysis\n",
    "    # final_transcript_text = \"\\n\".join(\n",
    "    #     f\"[{u['start']:.2f}s] {u['speaker']} ({u['lang_name']}): {u['text']}\"\n",
    "    #     for u in full_transcript\n",
    "    # )\n",
    "    \n",
    "    # Calculate token limits\n",
    "    max_input_tokens = context_window - reserved_tokens\n",
    "    transcript_tokens = estimate_tokens(final_transcript_text)\n",
    "    \n",
    "    print(f\"Transcript size: ~{transcript_tokens:,} tokens\")\n",
    "    print(f\"Model context window: {context_window:,} tokens\")\n",
    "    print(f\"Max input allowed: {max_input_tokens:,} tokens\")\n",
    "    \n",
    "    # Decide processing method\n",
    "    if transcript_tokens <= max_input_tokens:\n",
    "        print(\"\\n‚úì Transcript fits in context window - using direct processing\")\n",
    "        final_report = _direct_processing(final_transcript_text, client, model_name, target_language)\n",
    "        processing_method = \"Direct\"\n",
    "    else:\n",
    "        print(f\"\\n‚ö† Transcript exceeds context window by ~{transcript_tokens - max_input_tokens:,} tokens\")\n",
    "        print(\"Using Map-Reduce approach...\")\n",
    "        final_report = _map_reduce_processing(final_transcript_text, client, model_name, \n",
    "                                              max_input_tokens, context_window, target_language)\n",
    "        processing_method = \"Map-Reduce\"\n",
    "    \n",
    "    # Create metadata dictionary\n",
    "    metadata = {\n",
    "        'audio_filename': audio_filename,\n",
    "        'generated_by': generated_by,\n",
    "        'generation_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'model_name': model_name,\n",
    "        'transcript_tokens': transcript_tokens,\n",
    "        'processing_method': processing_method,\n",
    "        'target_language': target_language\n",
    "    }\n",
    "    \n",
    "    return final_report, transcript_tokens, processing_method, metadata\n",
    "\n",
    "\n",
    "def _direct_processing(transcript_text, client, model_name, target_language):\n",
    "    \"\"\"Process transcript directly without chunking.\"\"\"\n",
    "    direct_prompt = f\"\"\"You are an expert audio meeting analyst (InterPARES-Audio). Analyze the following complete meeting transcript which may contain multiple languages. \n",
    "\n",
    "Based on the conversation content, infer likely names or roles for each speaker (e.g., \"Sarah - Project Manager\", \"John - Developer\", etc.).\n",
    "\n",
    "Please provide a comprehensive report IN {target_language.upper()} with the following sections:\n",
    "\n",
    "1. **Executive Summary**: Provide a concise, high-level summary of the meeting's purpose, key discussion points, and major outcomes. When mentioning participants, use their inferred names where identified (e.g., \"SPEAKER_00, inferred as Maria\").\n",
    "\n",
    "2. **Speaker Profiles**: For each speaker, provide:\n",
    "   - Speaker ID (e.g., SPEAKER_00)\n",
    "   - Spoken Language(s)\n",
    "   - Predicted Name/Role (inferred from context)\n",
    "   - Individual Summary: What this person discussed, their main points, concerns, and contributions\n",
    "\n",
    "3. **Main Topics Discussed**: Key subjects and themes covered in the meeting.\n",
    "\n",
    "4. **Decisions Made**: Any conclusions or agreements reached.\n",
    "\n",
    "5. **Action Items**: A consolidated, numbered list of all tasks, recommendations, and follow-ups (specify who is responsible if mentioned).\n",
    "\n",
    "6. **Key Insights**: The top 3-5 most important insights or noteworthy observations.\n",
    "\n",
    "Meeting Transcript:\n",
    "---\n",
    "{transcript_text}\n",
    "---\n",
    "IMPORTANT: Provide only the report content without any signatures, sign-offs, or closing remarks at the end.\n",
    "\n",
    "Final Report (in {target_language}):\"\"\"\n",
    "\n",
    "    print(\"\\nGenerating comprehensive analysis...\")\n",
    "    response = client.chat(\n",
    "        model=model_name,\n",
    "        messages=[{'role': 'user', 'content': direct_prompt}]\n",
    "    )\n",
    "    \n",
    "    return response['message']['content']\n",
    "\n",
    "\n",
    "def _map_reduce_processing(transcript_text, client, model_name, max_input_tokens, context_window, target_language):\n",
    "    \"\"\"Process large transcript using map-reduce approach.\"\"\"\n",
    "    from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "    \n",
    "    # Calculate optimal chunk size\n",
    "    chunk_size_chars = (max_input_tokens // 2) * 4\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=chunk_size_chars,\n",
    "        chunk_overlap=400,\n",
    "        separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]\n",
    "    )\n",
    "    chunks = text_splitter.split_text(transcript_text)\n",
    "    print(f\"Split transcript into {len(chunks)} chunks\")\n",
    "    \n",
    "    # MAP STAGE: Analyze each chunk\n",
    "    map_prompt_template = \"\"\"You are an expert audio meeting analyst (InterPARES-Audio). Analyze the following segment of a meeting transcript which may contain multiple languages. \n",
    "\n",
    "For each speaker in this segment, try to infer their likely name or role from context clues.\n",
    "\n",
    "Extract:\n",
    "- **Speakers Present**: List each speaker with their ID, language(s) spoken, and predicted name/role\n",
    "- **Main Topics**: Key subjects discussed in this segment\n",
    "- **Decisions**: Any conclusions or agreements\n",
    "- **Action Items**: Specific tasks or recommendations (note who is responsible)\n",
    "- **Key Points**: Important information or insights\n",
    "\n",
    "Be concise but comprehensive.\n",
    "\n",
    "Transcript Segment:\n",
    "---\n",
    "{chunk_text}\n",
    "---\n",
    "\n",
    "Analysis:\"\"\"\n",
    "    \n",
    "    print(f\"\\n--- MAP STAGE: Processing {len(chunks)} chunks ---\")\n",
    "    chunk_outputs = []\n",
    "    for i, chunk in enumerate(chunks, 1):\n",
    "        chunk_tokens = estimate_tokens(chunk)\n",
    "        print(f\"Chunk {i}/{len(chunks)} (~{chunk_tokens:,} tokens)...\", end=\" \")\n",
    "        \n",
    "        prompt = map_prompt_template.format(chunk_text=chunk)\n",
    "        \n",
    "        try:\n",
    "            response = client.chat(\n",
    "                model=model_name,\n",
    "                messages=[{'role': 'user', 'content': prompt}]\n",
    "            )\n",
    "            chunk_outputs.append(f\"=== Chunk {i} Analysis ===\\n{response['message']['content']}\")\n",
    "            print(\"‚úì\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Error: {e}\")\n",
    "            chunk_outputs.append(f\"=== Chunk {i} Analysis ===\\n[Error processing chunk]\")\n",
    "    \n",
    "    # REDUCE STAGE: Synthesize all analyses\n",
    "    combined_chunk_outputs = \"\\n\\n\".join(chunk_outputs)\n",
    "    combined_tokens = estimate_tokens(combined_chunk_outputs)\n",
    "    \n",
    "    print(f\"\\n--- REDUCE STAGE: Synthesizing {combined_tokens:,} tokens ---\")\n",
    "    \n",
    "    # Hierarchical reduction if needed\n",
    "    if combined_tokens > max_input_tokens:\n",
    "        print(\"‚ö† Combined outputs too large, performing hierarchical reduction...\")\n",
    "        output_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=chunk_size_chars,\n",
    "            chunk_overlap=200\n",
    "        )\n",
    "        output_chunks = output_splitter.split_text(combined_chunk_outputs)\n",
    "        \n",
    "        intermediate_summaries = []\n",
    "        for i, output_chunk in enumerate(output_chunks, 1):\n",
    "            print(f\"Intermediate summary {i}/{len(output_chunks)}...\", end=\" \")\n",
    "            intermediate_prompt = f\"\"\"Synthesize the following meeting analyses. For each speaker, try to identify their name/role and summarize their contributions. Include topics, decisions, action items, and insights:\n",
    "\n",
    "{output_chunk}\n",
    "\n",
    "Synthesis:\"\"\"\n",
    "            response = client.chat(\n",
    "                model=model_name,\n",
    "                messages=[{'role': 'user', 'content': intermediate_prompt}]\n",
    "            )\n",
    "            intermediate_summaries.append(response['message']['content'])\n",
    "            print(\"‚úì\")\n",
    "        \n",
    "        combined_chunk_outputs = \"\\n\\n\".join(intermediate_summaries)\n",
    "    \n",
    "    # Final reduction\n",
    "    reduce_prompt_template = f\"\"\"You are a professional summarizer. You have been given analyses from a long, multilingual conversation. Synthesize this information into a comprehensive final report in {target_language}.\n",
    "\n",
    "Based on all the information, infer likely names or roles for each speaker mentioned.\n",
    "\n",
    "Provide the following sections IN {target_language.upper()}:\n",
    "\n",
    "1. Executive Summary: Provide a concise, high-level summary of the meeting's purpose, key discussion points, and major outcomes. When mentioning participants, use their inferred names where identified (e.g., \"SPEAKER_00, inferred as Maria\").\n",
    "\n",
    "2. **Speaker Profiles**: For each speaker identified across all segments, provide:\n",
    "   Format: Speaker XX (Spoken Language: XX, Predicted Name: XXX): [Summary of what they discussed]\n",
    "   Include their main points, concerns, and contributions to the conversation.\n",
    "\n",
    "3. **Main Topics Discussed**: Key subjects and themes covered.\n",
    "\n",
    "4. **Decisions Made**: Conclusions or agreements reached during the meeting.\n",
    "\n",
    "5. **Action Items**: A consolidated, numbered list of all tasks and recommendations (specify who is responsible if mentioned).\n",
    "\n",
    "6. **Key Insights**: The top 3-5 most important insights or noteworthy observations.\n",
    "\n",
    "Information from Conversation Segments:\n",
    "---\n",
    "{combined_chunk_outputs}\n",
    "---\n",
    "IMPORTANT: Provide only the report content without any signatures, sign-offs, or closing remarks at the end.\n",
    "\n",
    "Final Report (in {target_language}):\"\"\"\n",
    "    \n",
    "    print(\"Generating final report...\")\n",
    "    final_response = client.chat(\n",
    "        model=model_name,\n",
    "        messages=[{'role': 'user', 'content': reduce_prompt_template}]\n",
    "    )\n",
    "    \n",
    "    return final_response['message']['content']\n",
    "\n",
    "\n",
    "def save_report(final_report, metadata, output_dir=\"meeting_reports\", base_filename=None, logo_path=None):\n",
    "    \"\"\"\n",
    "    Save the meeting report to Markdown and PDF files.\n",
    "    \n",
    "    Args:\n",
    "        final_report (str): The generated report content\n",
    "        metadata (dict): Dictionary containing:\n",
    "            - audio_filename (str): Original audio file name\n",
    "            - generated_by (str): Person who generated the report\n",
    "            - generation_date (str): Date/time of generation\n",
    "            - model_name (str): Model used\n",
    "            - transcript_tokens (int): Token count\n",
    "            - processing_method (str): Processing method used\n",
    "        output_dir (str): Directory to save reports\n",
    "        base_filename (str): Base filename (default: timestamped)\n",
    "        logo_path (str): Path to logo image file (optional, supports PNG, JPG, SVG)\n",
    "    \n",
    "    Returns:\n",
    "        dict: Paths to saved files {'markdown': path, 'pdf': path or None}\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Saving Report ---\")\n",
    "    \n",
    "    # Create output directory\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Generate filename\n",
    "    if base_filename is None:\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        base_filename = f\"meeting_analysis_{metadata['audio_filename'].split('/')[-1].split('.')[0]}-{timestamp}-{metadata['target_language']}\"\n",
    "    \n",
    "    saved_files = {}\n",
    "    \n",
    "    # Save as Markdown\n",
    "    md_file = os.path.join(output_dir, f\"{base_filename}.md\")\n",
    "    with open(md_file, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"# UNESCO Meeting Analysis Report\\n\\n\")\n",
    "        f.write(\"## Metadata\\n\\n\")\n",
    "        if metadata.get('audio_filename'):\n",
    "            f.write(f\"**Audio File:** {metadata['audio_filename']}\\n\\n\")\n",
    "        f.write(f\"**Date of Transcript:** {metadata['generation_date']}\\n\\n\")\n",
    "        f.write(f\"**Target Language:** {metadata['target_language']}\\n\\n\")\n",
    "        # f.write(f\"**Model:** {metadata['model_name']}\\n\\n\")\n",
    "        f.write(f\"**Transcript Length:** ~{metadata['transcript_tokens']:,} tokens\\n\\n\")\n",
    "        # f.write(f\"**Processing Method:** {metadata['processing_method']}\\n\\n\")\n",
    "        if metadata.get('generated_by'):\n",
    "            f.write(f\"**Generated By:** {metadata['generated_by']}\\n\\n\")\n",
    "        f.write(\"---\\n\\n\")\n",
    "        f.write(final_report)\n",
    "    \n",
    "    print(f\"‚úì Markdown report saved to: {md_file}\")\n",
    "    saved_files['markdown'] = md_file\n",
    "    \n",
    "    # Save as PDF\n",
    "    pdf_file = os.path.join(output_dir, f\"{base_filename}.pdf\")\n",
    "    pdf_saved = _save_as_pdf(final_report, pdf_file, metadata, logo_path)\n",
    "    \n",
    "    if pdf_saved:\n",
    "        saved_files['pdf'] = pdf_file\n",
    "    else:\n",
    "        saved_files['pdf'] = None\n",
    "    \n",
    "    return saved_files\n",
    "\n",
    "def _save_as_pdf(report_content, pdf_path, metadata, logo_path=None):\n",
    "    \"\"\"Save report as PDF with styling.\"\"\"\n",
    "    \n",
    "    # --- FIX: Sanitize text to prevent character encoding errors in PDF ---\n",
    "    if \"Arabic\" not in metadata['target_language']:\n",
    "        report_content = report_content.encode('ascii', 'ignore').decode('utf-8')\n",
    "    # --------------------------------------------------------------------\n",
    "\n",
    "    try:\n",
    "        # Try weasyprint first (recommended)\n",
    "        try:\n",
    "            import markdown2\n",
    "            from weasyprint import HTML as WeasyHTML\n",
    "            import base64\n",
    "            \n",
    "            # Prepare logo data if provided\n",
    "            logo_data_uri = None\n",
    "            if logo_path and os.path.exists(logo_path):\n",
    "                with open(logo_path, 'rb') as img_file:\n",
    "                    img_data = base64.b64encode(img_file.read()).decode('utf-8')\n",
    "                    # Detect image type\n",
    "                    ext = os.path.splitext(logo_path)[1].lower()\n",
    "                    mime_types = {\n",
    "                        '.png': 'image/png',\n",
    "                        '.jpg': 'image/jpeg',\n",
    "                        '.jpeg': 'image/jpeg',\n",
    "                        '.svg': 'image/svg+xml',\n",
    "                        '.gif': 'image/gif'\n",
    "                    }\n",
    "                    mime_type = mime_types.get(ext, 'image/png')\n",
    "                    logo_data_uri = f\"data:{mime_type};base64,{img_data}\"\n",
    "            \n",
    "            html_content = markdown2.markdown(report_content, extras=['tables', 'fenced-code-blocks'])\n",
    "            styled_html = _create_styled_html(html_content, metadata, logo_data_uri)\n",
    "            \n",
    "            WeasyHTML(string=styled_html).write_pdf(pdf_path)\n",
    "            print(f\"‚úì PDF report saved to: {pdf_path}\")\n",
    "            return True\n",
    "            \n",
    "        except ImportError:\n",
    "            # Fallback to pdfkit\n",
    "            try:\n",
    "                import markdown2\n",
    "                import pdfkit\n",
    "                import base64\n",
    "                \n",
    "                # Prepare logo data if provided\n",
    "                logo_data_uri = None\n",
    "                if logo_path and os.path.exists(logo_path):\n",
    "                    with open(logo_path, 'rb') as img_file:\n",
    "                        img_data = base64.b64encode(img_file.read()).decode('utf-8')\n",
    "                        ext = os.path.splitext(logo_path)[1].lower()\n",
    "                        mime_types = {\n",
    "                            '.png': 'image/png',\n",
    "                            '.jpg': 'image/jpeg',\n",
    "                            '.jpeg': 'image/jpeg',\n",
    "                            '.svg': 'image/svg+xml',\n",
    "                            '.gif': 'image/gif'\n",
    "                        }\n",
    "                        mime_type = mime_types.get(ext, 'image/png')\n",
    "                        logo_data_uri = f\"data:{mime_type};base64,{img_data}\"\n",
    "                \n",
    "                html_content = markdown2.markdown(report_content, extras=['tables', 'fenced-code-blocks'])\n",
    "                styled_html = _create_styled_html(html_content, metadata, logo_data_uri)\n",
    "                \n",
    "                pdfkit.from_string(styled_html, pdf_path)\n",
    "                print(f\"‚úì PDF report saved to: {pdf_path}\")\n",
    "                return True\n",
    "                \n",
    "            except ImportError:\n",
    "                print(\"‚ö† PDF generation skipped: Install 'weasyprint' or 'pdfkit + wkhtmltopdf'\")\n",
    "                print(\"  pip install weasyprint\")\n",
    "                print(\"  OR\")\n",
    "                print(\"  pip install pdfkit  # also requires wkhtmltopdf binary\")\n",
    "                return False\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö† PDF generation failed: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "\n",
    "def _create_styled_html(content, metadata, logo_data_uri=None):\n",
    "    \"\"\"Create styled HTML for PDF generation.\"\"\"\n",
    "    # Create logo HTML if provided\n",
    "    logo_html = \"\"\n",
    "    if logo_data_uri:\n",
    "        logo_html = f'<img src=\"{logo_data_uri}\" alt=\"Logo\" class=\"logo\" >'\n",
    "    \n",
    "    metadata_html = \"\"\n",
    "    if metadata.get('audio_filename'):\n",
    "        metadata_html += f\"<strong>Audio File:</strong> {metadata['audio_filename']}<br>\\n            \"\n",
    "    metadata_html += f\"<strong>Date of Transcript:</strong> {metadata['generation_date']}<br>\\n            \"\n",
    "    metadata_html += f\"<strong>Target Language:</strong> {metadata['target_language']}<br>\\n            \"\n",
    "    # metadata_html += f\"<strong>Model:</strong> {metadata['model_name']}<br>\\n            \"\n",
    "    metadata_html += f\"<strong>Transcript Length:</strong> ~{metadata['transcript_tokens']:,} tokens            \"\n",
    "    # metadata_html += f\"<strong>Processing Method:</strong> {metadata['processing_method']}\"\n",
    "    if metadata.get('generated_by'):\n",
    "        metadata_html += f\"<br>\\n            <strong>Generated By:</strong> {metadata['generated_by']}\"\n",
    "    \n",
    "    # Determine text direction based on target language\n",
    "    target_lang = metadata.get('target_language', 'English').lower()\n",
    "    rtl_languages = ['arabic', 'hebrew', 'persian', 'urdu', 'farsi', 'pashto']\n",
    "    is_rtl = any(lang in target_lang for lang in rtl_languages)\n",
    "    text_direction = 'rtl' if is_rtl else 'ltr'\n",
    "    text_align = 'right' if is_rtl else 'left'\n",
    "    # print (logo_html)\n",
    "    return f\"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html dir=\"{text_direction}\">\n",
    "    <head>\n",
    "        <meta charset=\"utf-8\">\n",
    "        <style>\n",
    "            @page {{\n",
    "                size: A4;\n",
    "                margin: 1.5cm;\n",
    "            }}\n",
    "            body {{\n",
    "                font-family: 'Segoe UI', 'Tahoma', 'Arial', 'Noto Sans Arabic', 'Traditional Arabic', 'Simplified Arabic', sans-serif;\n",
    "                line-height: 1.4;\n",
    "                color: #333;\n",
    "                font-size: 10pt;\n",
    "                margin: 0;\n",
    "                padding: 0;\n",
    "                direction: {text_direction};\n",
    "                text-align: {text_align};\n",
    "            }}\n",
    "    \n",
    "            .header-container {{\n",
    "                border-bottom: 2px solid #1c3b7c;\n",
    "                padding-bottom: 10px;\n",
    "                margin-bottom: 20px;\n",
    "                display: flex;\n",
    "                flex-direction: column;\n",
    "                align-items: flex-start;\n",
    "            }}\n",
    "    \n",
    "            .logo {{\n",
    "                max-height: 70px;\n",
    "                max-width: 220px;\n",
    "                margin-bottom: 8px;\n",
    "                \n",
    "            }}\n",
    "    \n",
    "            .title {{\n",
    "                width: 100%;\n",
    "                text-align: center;\n",
    "                font-size: 26px;\n",
    "                font-weight: 700;\n",
    "                color: #1c3b7c;\n",
    "            }}\n",
    "    \n",
    "            h2 {{\n",
    "                color: #607e2c;\n",
    "                margin-top: 16px;\n",
    "                margin-bottom: 8px;\n",
    "                border-bottom: 1px solid #607e2c;\n",
    "                padding-bottom: 4px;\n",
    "                font-size: 13pt;\n",
    "                text-align: {text_align};\n",
    "            }}\n",
    "    \n",
    "            h3 {{\n",
    "                color: #7f8c8d;\n",
    "                margin-top: 12px;\n",
    "                margin-bottom: 6px;\n",
    "                font-size: 11pt;\n",
    "                text-align: {text_align};\n",
    "            }}\n",
    "    \n",
    "            ul, ol {{\n",
    "                margin-{text_align}: 18px;\n",
    "                margin-top: 6px;\n",
    "                margin-bottom: 6px;\n",
    "            }}\n",
    "    \n",
    "            li {{\n",
    "                margin-bottom: 4px;\n",
    "                line-height: 1.3;\n",
    "                text-align: {text_align};\n",
    "            }}\n",
    "    \n",
    "            p {{\n",
    "                margin-top: 4px;\n",
    "                margin-bottom: 6px;\n",
    "                text-align: {text_align};\n",
    "            }}\n",
    "    \n",
    "            strong {{\n",
    "                color: #2c3e50;\n",
    "            }}\n",
    "    \n",
    "            .metadata {{\n",
    "                background-color: #ecf0f1;\n",
    "                padding: 10px 12px;\n",
    "                border-radius: 4px;\n",
    "                margin-bottom: 16px;\n",
    "                font-size: 9pt;\n",
    "                line-height: 1.3;\n",
    "                text-align: left;\n",
    "                direction: ltr;\n",
    "            }}\n",
    "    \n",
    "            code {{\n",
    "                background-color: #f7f7f7;\n",
    "                padding: 1px 4px;\n",
    "                border-radius: 2px;\n",
    "                font-family: 'Courier New', monospace;\n",
    "                font-size: 9pt;\n",
    "            }}\n",
    "    \n",
    "            table {{\n",
    "                border-collapse: collapse;\n",
    "                width: 100%;\n",
    "                margin: 8px 0;\n",
    "                font-size: 9pt;\n",
    "                direction: {text_direction};\n",
    "            }}\n",
    "    \n",
    "            th, td {{\n",
    "                border: 1px solid #ddd;\n",
    "                padding: 6px 8px;\n",
    "                text-align: {text_align};\n",
    "            }}\n",
    "    \n",
    "            th {{\n",
    "                background-color: #f2f2f2;\n",
    "                font-weight: bold;\n",
    "            }}\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "    \n",
    "        <div class=\"header-container\">\n",
    "            <div class=\"logo\">{logo_html}</div>\n",
    "            <div class=\"title\">üìã UNESCO Meeting Analysis Report</div>\n",
    "        </div>\n",
    "    \n",
    "        <div class=\"metadata\">\n",
    "            {metadata_html}\n",
    "        </div>\n",
    "    \n",
    "        {content}\n",
    "    \n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c850a20-b116-4ce4-b394-33f7d8118ba1",
   "metadata": {},
   "source": [
    "### ‚úÖ Step 5: Process Audio and Finalize Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fed36f-dabf-4100-abd5-be799f11d095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model configuration\n",
    "MODEL_NAME = \"gpt-oss:20b\"\n",
    "CONTEXT_WINDOW = 8192  # Confirmed from Ollama logs\n",
    "RESERVED_TOKENS = 1500\n",
    "GENERATED_BY = \"InterPARES-Audio\"  # Replace with your name or identifier\n",
    "\n",
    "# Metadata configuration\n",
    "for fname in [\n",
    "                #\"A02898\", \n",
    "                \"A02997\", \"A03046\", \"A03520\", \"A03858\", \"A04616\", \"A04957\", \"A04960\", \n",
    "                        \"A04965\", \"A05012\", \"A05195\", \"A05857\", \"A06010\", \"A06118\", \"A06622\", \"A06850\",\n",
    "                        \"A06888\", \"A06918\", \"A06936\", \"A07167\", \"A07251\", \"A07296\", \"A07452\", \"A07462\", \n",
    "                        \"A07679\", \"A07768\", \"A07791\", \"A08133\", \"A08649\", \"A08957\", \"A08958\", \"A08960\", \n",
    "                        \"A08966\", \"A09304\"]:\n",
    "    AUDIO_FILENAME = f\"resampled_audio/{fname}/{fname}.wav\"\n",
    "    OUTPUT_DIR=f\"meeting_reports/{AUDIO_FILENAME.split('/')[-1][:-4]}\"\n",
    "    # Step 1: Generate the utterances\n",
    "    diarization_output = generate_utterances(AUDIO_FILENAME)\n",
    "    # Step e: Transcribe the utterances the report\n",
    "    final_transcript_text = transcribe(AUDIO_FILENAME)\n",
    "    # Step 3: Generate the report in multiple languages\n",
    "    for TARGET_LANGUAGE in [\"Arabic\", \"English\", \"French\", \"Spanish\", \"German\", \"Italian\"]:\n",
    "        print(f\"***** {TARGET_LANGUAGE} *****\")\n",
    "        final_report, transcript_tokens, processing_method, metadata = generate_meeting_report(\n",
    "            final_transcript_text=final_transcript_text,\n",
    "            client=client,\n",
    "            model_name=MODEL_NAME,\n",
    "            context_window=CONTEXT_WINDOW,\n",
    "            reserved_tokens=RESERVED_TOKENS,\n",
    "            audio_filename=AUDIO_FILENAME,\n",
    "            generated_by=GENERATED_BY,\n",
    "            target_language=TARGET_LANGUAGE\n",
    "        )\n",
    "        # Step 4: Save the report\n",
    "        saved_files = save_report(\n",
    "            final_report=final_report,\n",
    "            metadata=metadata,\n",
    "            output_dir=f\"meeting_reports/{AUDIO_FILENAME.split('/')[-1][:-4]}\",\n",
    "            logo_path=\"InterPARES_Audio.jpg\"  # Path to your logo file\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"Report generation complete!\")\n",
    "        print(f\"Markdown: {saved_files['markdown']}\")\n",
    "        if saved_files['pdf']:\n",
    "            print(f\"PDF: {saved_files['pdf']}\")\n",
    "        with open(f\"{OUTPUT_DIR}/final_transcript_text.txt\", \"a\") as tf:\n",
    "            print(final_transcript_text, file=tf)\n",
    "        print(f\"{'='*70}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
